# Neural Network Implementation from Scratch
This project is a from-scratch implementation of a neural network in Python, using only numpy for numerical computations. It includes modules for layers (e.g., dense layers), activation functions (e.g., ReLU, Sigmoid, Softmax), and loss functions (e.g., Categorical Crossentropy, Mean Squared Error). The implementation is modular, allowing for easy extension and experimentation.

## Features
### Layers:
- Dense_Layer: Fully connected layer with trainable weights and biases.

### Activation Functions:

- Relu: Rectified Linear Unit (ReLU) activation function.

- Sigmoid: Sigmoid activation function.

- Softmax: Softmax activation function for multi-class classification.

### Loss Functions:

- Categorical_Crossentropy_Loss: Loss function for multi-class classification.

- MSE_Loss: Mean Squared Error loss for regression tasks.

### Training:

- Stochastic Gradient Descent (SGD) with momentum for weight updates.
  Modular design for forward and backward propagation.
